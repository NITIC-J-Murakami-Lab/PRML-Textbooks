{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN（Recurrent Neural Network, 再帰型ニューラルネットワーク）\n",
    "\n",
    "これから，系列データをニューラルネットワークで扱う方法について考えていきます．たとば系列 $\\mathbf{x}=( x_1,x_2, \\cdots,x_t, \\cdots, x_{T} ) \\in \\mathbb{R}^T$ を用意します．これは例えば\n",
    "\n",
    "1. 株価の値動きのデータ系列\n",
    "2. 毎日同じ時間に記録した一関市の気温\n",
    "\n",
    "などが考えられます．なんでも大丈夫です．このデータを使って，\n",
    "\n",
    "1. $\\mathbf{x}_{\\text{input}}^{(0)}=(x_0)$を使って$x_{1}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(1)}=(x_0,x_1)$を使って$x_{2}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(i)}=(x_0,x_1,\\cdots,x_t)$を使って$x_{t+1}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(T-1)}=(x_0,x_1,\\cdots, x_t,\\cdots x_{T-1})$を使って$x_{T}$を予測する\n",
    "\n",
    "ような __回帰__ タスクを考えます．通常のMLPでこれを再現するには，適当な最大系列長（ここでは$|S|-1$）を設けて，入力系列の不足分を全て0-paddingすることが考えられます．  \n",
    "つまり：\n",
    "1. $\\mathbf{x}_{\\text{input}}^{(0)}=(x_0,0,\\cdots,0)$を使って$x_{1}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(1)}=(x_0,x_1,0,\\cdots,0)$を使って$x_{2}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(i)}=(x_0,x_1,\\cdots, x_t,0,\\cdots,0)$を使って$x_{t+1}$を予測する\n",
    "2. $\\mathbf{x}_{\\text{input}}^{(T-1)}=(x_0,x_1,\\cdots, x_t,\\cdots, x_{T-1})$を使って$x_{T}$を予測する\n",
    "\n",
    "こういうことになります．つまり以下のような入出力を持つMLPです．\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\begin{bmatrix}\n",
    "\\hat{x}_{1} \\\\\n",
    "\\hat{x}_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{x}_{T} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\text{MLP}(\\begin{bmatrix}\n",
    "\\mathbf{x}_{\\text{input}}^{(0)}\\\\\n",
    "\\mathbf{x}_{\\text{input}}^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_{\\text{input}}^{(T-1)} \\\\\n",
    "\\end{bmatrix}) \\\\\n",
    "\\Leftrightarrow & \\begin{bmatrix}\n",
    "\\hat{x}_{1} \\\\\n",
    "\\hat{x}_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\hat{x}_{T} \\\\\n",
    "\\end{bmatrix} =\n",
    "\\text{MLP}(\\begin{bmatrix}\n",
    "x_0 & 0 & \\cdots & 0 \\\\\n",
    "x_0 & x_1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0 & x_1 & \\cdots & x_{T-1} \\\\\n",
    "\\end{bmatrix})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "これでも問題ないように見えますが，効率が悪いですし，データが複数の特徴量を持っている場合は一気に複雑になってしまいます．なので，系列データの扱いに特化した構造（アーキテクチャ）を持つニューラルネットワークが求められました．そこで登場したのが __RNN（Recurrent Neural Network, 再帰型ニューラルネットワーク）__ です．\n",
    "\n",
    "\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/n/nkdkccmbr/20161006/20161006221349.png)  \n",
    "RNNのイメージ 出典：[３層パーセプトロン -- ニューラルネットワーク・DeepLearningなどの画像素材　プレゼン・ゼミなどに【WTFPL】](https://nkdkccmbr.hateblo.jp/entry/2016/10/06/222245)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パッケージのimport "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# packageのimport\n",
    "from typing import Any, Union, Callable, Type, TypeVar\n",
    "from tqdm.std import trange,tqdm\n",
    "import numpy as np \n",
    "import numpy.typing as npt\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "plt.style.use(\"bmh\")\n",
    "\n",
    "# pytorch関連のimport\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNとは\n",
    "\n",
    "RNNとは，時系列構造をもつデータをニューラルネットワークでうまく扱うことができる枠組みです．ネットワークの中に閉路があるだけでRNNと分類されるので，順伝搬において信号の伝搬路の中にループがあれば全てRNNと言われます．ここでは以下のようなネットワーク（エルマン型）を紹介します．  \n",
    "\n",
    "> ![](https://www.acceluniverse.com/blog/developers/LSimage_2.png)\n",
    "RNNのイメージ  \n",
    "出典：[Long Short-Term Memory](http://axon.cs.byu.edu/~martinez/classes/778/Papers/lstm.pdf)\n",
    "\n",
    "※$x$の添字が0からスタートするか1からスタートするのかについてはあまり頓着せずに，左の図を展開すると右になることに納得してほしいです．\n",
    "\n",
    "\n",
    "時系列データ$\\mathbf{x}=( x_1,x_2, \\cdots,x_t, \\cdots, x_{T} ) \\in \\mathbb{R}^T$が与えられた時，$x_t$までの情報を使って$x_{t+1}$を予測したい場合や，$\\mathbf{x}$に対応した別の時系列データ$\\mathbf{y}$を予測したい場合などが考えられます．\n",
    "\n",
    "回帰ならば次の値，クラス分類ならば何かしらのラベルを予測するわけですが，とりあえず「$x_{t-1}$までの情報を使って$o_{t}$を予測したい場合」としましょう．RNNでは$t-1$時点のMLPの中間層の出力を$t$時点のMLPの入力に利用します．__RNNでは単位時間ごとのMLPは全てパラメータを共有しており__，これまでと異なるのは$t$時点の隠れ層で\n",
    "$$h_t=(x_{t}\\cdot {W^{(x,h)}}^{\\top} + h_{t-1}\\cdot W^{(h,h)}+b)$$  \n",
    "が計算され，これが次の隠れ層の入力となることだけです．\n",
    "\n",
    "![](./figs/rnn/elman_arch_x0.png)\n",
    "\n",
    "つまり$h_t$は活性化関数がtanhの時，以下のようにして求められます：\n",
    "\n",
    "$$\n",
    "h_{t} := \\operatorname{tanh}(x_{t}\\cdot {W^{(x,h)}}^{\\top} + h_{t-1}\\cdot W^{(h,h)}+b)\n",
    "$$\n",
    "\n",
    "ただし，$h_0$にはゼロ行列を使います．  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} tanh\n",
    "\n",
    "ここで使われているtanhをはじめとした双曲線関数 $\\sinh$ （ハイパボリックサイン）, $\\cosh$（ハイパボリックコサイン）,$\\tanh$（ハイパボリックタンジェント） は以下の式で定義されます：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sinh x & =\\frac{e^x-e^{-x}}{2} \\\\\n",
    "\\cosh x & =\\frac{e^x+e^{-x}}{2} \\\\\n",
    "\\tanh x & =\\frac{\\sinh x}{\\cosh x}=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "特にtanhに着目すると，これはsigmoid関数に非常によく似た形になっている（取り得る範囲が0~1と-1~1の違い）ことがわかります．そのため，ニューラルネットワークの実装ではsigmoidの代わりにtanhを利用することもよくあります．\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5754, -0.1390,  0.0491, -0.5315, -0.9641],\n",
      "        [ 0.8867,  0.9431,  0.8443, -0.8073, -0.4357]])\n",
      "tensor([[-0.5754, -0.1390,  0.0491, -0.5315, -0.9641],\n",
      "        [ 0.8867,  0.9431,  0.8443, -0.8073, -0.4357]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.normal(0,1,size=[2,5])\n",
    "\n",
    "tanh_layer = nn.Tanh()\n",
    "print(tanh_layer(x))\n",
    "# or \n",
    "print(torch.tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フォワードプロパゲーションの流れ\n",
    "\n",
    "上述した通り，ここで紹介するRNNでは時間方向の影響をモデル化した隠れ層を利用しています．これを使ったRNNフォワードプロパゲーションの全体像を見ておきましょう．$f$と$g$は任意の活性化関数です．\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\text{input layer: } \\quad  x_t \\\\\n",
    "&\\text{hidden layer:} \\quad h_{t} = \\operatorname{f}(x_{t}\\cdot {W^{(x,h)}} + h_{t-1}\\cdot W^{(h,h)}+b^{x,h}) \\\\\n",
    "&\\text{output layer:} \\quad o_{t} = \\operatorname{g}(h_t \\cdot W^{(h,o)}+b^{(h,o)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    ":::{hint}\n",
    "\n",
    "例えば文書生成などがRNNを使う例として考えられます．入力データとして，ある文書に登場する単語一つ一つを$x_1,x_2,\\cdots,x_{T-1}$として入力して，それぞれの出力を$\\hat{x_2},\\hat{x_3},\\cdots, \\hat{x_T}$としましょう．この時出力値はそれぞれ「入力された単語の次に出現する単語」に相当します．\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNの疑似コード {prf:ref}`rnn-forward-prop`を作るために，以下の様に定義します．\n",
    "\n",
    "- $M$: バッチサイズ\n",
    "- $T$: 時間\n",
    "- $K$: 特徴数\n",
    "- $C$: クラス数\n",
    "- $H$: 隠れ層の次元数\n",
    "\n",
    "\n",
    "また，$\\operatorname{softmax}$は最もネストの深い軸（axis=-1）を基準に計算しているとします．\n",
    "\n",
    "::::{prf:algorithm} Forward propagation of RNN\n",
    ":label: rnn-forward-prop\n",
    "\n",
    "**Input:** $\\mathbf{X} \\in \\mathbb{R}^{M \\times T \\times K}$: mini-batch データ, $\\Theta$: モデルパラメータの集合\n",
    "\n",
    "ここで, $\\Theta$には以下のパラメータが含まれている．\n",
    "\n",
    "- $\\mathbf{W}^{(x,h)} \\in \\mathbb{R}^{K \\times H}$: 入力層→隠れ層の重み\n",
    "- $\\mathbf{W}^{(h,h)} \\in \\mathbb{R}^{H \\times H}$: 隠れ層→隠れ層の重み\n",
    "- $\\mathbf{W}^{(h,o)} \\in \\mathbb{R}^{H \\times C}$: 隠れ層→出力層の重み\n",
    "- $\\mathbf{b}^{(x,h)} \\in \\mathbb{R}^{H}$: 入力層→隠れ層のバイアス\n",
    "- $\\mathbf{b}^{(h,o)} \\in \\mathbb{R}^{C}$: 隠れ層→出力層のバイアス\n",
    "\n",
    "**Output:** $\\hat{\\mathbf{Y}} \\in \\mathbb{R}_{>0}^{M \\times T \\times C}$: 各クラスへの所属確率\n",
    "\n",
    "1. 全ての要素を0で初期化 \n",
    "    $\\mathbf{H} \\leftarrow \\mathbf{O} \\in \\mathbb{R}^{M \\times T \\times H}$\n",
    "2. $\\hat{\\mathbf{H}}_{0} \\leftarrow \\mathbf{O} \\in \\mathbb{R}^{M \\times H}$\n",
    "3. **For** $t = 1:T$ **do**:\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;$t$時点の隠れ層出力を算出 $\\hat{\\mathbf{H}}_{t} \\leftarrow \\operatorname{tanh}(\\mathbf{X}_{:,t,:} \\cdot \\mathbf{W}^{(x,h)}+\\mathbf{h}_{t-1}\\cdot \\mathbf{W}^{(h,h)} + \\mathbf{b}^{(x,h)})$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;全ての時点の出力を一つの配列に保存 $\\mathbf{H}_{:,t,:} \\leftarrow \\hat{\\mathbf{H}}_{t}$     \n",
    "6. 出力を算出\n",
    "    $\\hat{\\mathbf{Y}} \\leftarrow \\operatorname{softmax}(\\mathbf{H} \\cdot  \\mathbf{W}^{(h,o)} + \\mathbf{b}^{(h,o)})$\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{admonition} RNN Graph Game\n",
    ":class: tips\n",
    "\n",
    "順伝搬のフローが理解できたか確認するために，[Graph Game](https://graphgame.sabrina.dev/rnn)をやってみましょう．\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT（Backpropagation through time）\n",
    "\n",
    "学習ではunfold（展開）したネットワークに対して，一番最後の出力から遡ってBack Propagationしていきます．この手法のことを __Back Propagation Through Time（BPTT）__ と呼びます．\n",
    "\n",
    "> ![](https://camo.qiitausercontent.com/b0c4040112f747cc634270019bfb3b91b0f88f6e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3536383638382f33396539396631322d303561632d383831312d663663332d6461383962316538396565362e706e67)  \n",
    "[出典: PyTorchを使ってジャパリパークの歌詞みたいなやつを生成させたい](https://qiita.com/kibounoasa/items/78fb5a217232de863b00)\n",
    "\n",
    "\n",
    "パラメータの更新は以下のように行います：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{w}_{\\mathrm{ji}}^{(\\mathrm{l}) \\text { new }} & =\\mathrm{w}_{\\mathrm{ji}}^{(\\mathrm{l}) \\text { old }}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{W}_{\\mathrm{ji}}^{(l)}} \\\\\n",
    "\\mathrm{w}_{\\mathrm{jj}}^{(\\mathrm{l}) \\text { new }} & =\\mathrm{w}_{\\mathrm{jj}}^{(\\mathrm{l}) \\text {,old }}-\\eta \\frac{\\partial \\mathrm{L}}{\\partial \\mathrm{w}_{\\mathrm{jj}}^{(l)}} \\\\\n",
    "\\mathrm{L} & =\\sum_{\\mathrm{t}=1}^T \\mathrm{~L}^{(\\mathrm{t})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "損失関数は全ての出力値から計算したものを合計して利用します．それ以外は普通のMLPと同様です．\n",
    "\n",
    "- BPTTの利点\n",
    "    - これまでのMLPの訓練方法の通りである\n",
    "- BPTTの欠点\n",
    "    - 計算グラフが大きくなるので勾配消失問題や勾配爆発問題が起こりやすい\n",
    "    \n",
    "この問題を解決するために，ある程度遡ったら途中で勾配を切り捨てる様なBPTTの拡張として，__Truncated BPTT__ が提案されています．\n",
    "\n",
    "> ![](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F568688%2Faace0b89-3603-bfcd-5cbe-307f55fc0049.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=779bd53f95eef72ccd37d19819cd4c69)  \n",
    "[出典: PyTorchを使ってジャパリパークの歌詞みたいなやつを生成させたい](https://qiita.com/kibounoasa/items/78fb5a217232de863b00)\n",
    "\n",
    "\n",
    "遡るタイムステップ数 $\\tau$ を限定したBPTTを Truncated BPTT といいます。\n",
    "\n",
    "- Truncated BPTTの利点\n",
    "    - 勾配消失や勾配爆発をある程度抑制できる．\n",
    "- Truncated BPTTの欠点\n",
    "    - 任意の範囲で勾配履歴を切り落としてしまう．\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例：文書生成モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データローダー\n",
    "\n",
    ":::{note}\n",
    "\n",
    "wikipediaの一部をNLPに使いやすい形でまとめたデータセットがtext8です．これの日本語版を有志の方が公開してくれています．\n",
    "\n",
    "[ja.text8](https://github.com/Hironsan/ja.text8)\n",
    "\n",
    ":::\n",
    "\n",
    "これをダウンロードして，unzipしてください．それができたら次のコードの`PATH`にこのファイルのpathを指定します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def build_simply_dictionary(texts):\n",
    "    token_set = set(token for text in texts for token in text.split())\n",
    "    word2id = {token:id for id, token in enumerate(token_set)}  \n",
    "    return word2id\n",
    "\n",
    "def my_analyzer(text):\n",
    "    #text = code_regex.sub('', text)\n",
    "    tokens = text.split()\n",
    "    tokens = filter(lambda token: re.search(r'[ぁ-ん]+|[ァ-ヴー]+|[一-龠]+', token), tokens)\n",
    "    return tokens \n",
    "\n",
    "def build_dictionary(texts, min_df=1):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    countvectorizer = CountVectorizer(min_df=min_df, analyzer=my_analyzer)\n",
    "\n",
    "    X = countvectorizer.fit_transform(texts)\n",
    "    id2word = {id:w for id,w in enumerate(countvectorizer.get_feature_names_out())}\n",
    "    word2id = {w:id for id,w in id2word.items()}\n",
    "    return id2word, word2id, X\n",
    "\n",
    "PATH = \"./data/ja.text8\"\n",
    "\n",
    "with open(PATH) as f:\n",
    "    text8 = f.read()\n",
    "\n",
    "texts = text8.split(\"。\")\n",
    "id2word, word2id,X = build_dictionary(texts,5)\n",
    "V = len(id2word)\n",
    "D = len(texts)\n",
    "print(f\"文書数: {D}, 語彙数: {V}\")\n",
    "\n",
    "WINDOW_SIZE = 11\n",
    "\n",
    "preprocessed_texts = [[word2id[w] for w in text.split() if w in word2id] for text in texts]\n",
    "preprocessed_texts = [text for text in preprocessed_texts if len(text) > WINDOW_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word2id_seq(corpus ,word2id, max_length=10):\n",
    "    tmp = []\n",
    "    for doc in corpus:\n",
    "        line = [word2id[w] for w in doc.split() if w in word2id]\n",
    "        # もしmax_lengthよりも単語数が多ければ切り捨て，\n",
    "        if len(line) >= max_length:\n",
    "            line = line[:max_length]\n",
    "        # もしmax_lengthよりも単語数が少なければ0で穴埋め,\n",
    "        elif len(line) <= max_length:\n",
    "            line += [0]* (max_length - len(line))\n",
    "        tmp.append(line)\n",
    "    return tmp \n",
    "\n",
    "def make_id2word_seq(id_seqs, id2word):\n",
    "    tmp = []\n",
    "    for id_seq in id_seqs:\n",
    "        line = [id2word[id] for id in id_seq  if id in id2word]\n",
    "        tmp.append(\" \".join(line))\n",
    "    return tmp\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, id_data):\n",
    "        super().__init__()\n",
    "        self.data_length = len(id_data)\n",
    "        # 訓練データ。例：［'僕', 'は', 'カレー', 'が', '好き']\n",
    "        self.x = [row[0:-1] for row in id_data]\n",
    "        # 正解ラベル。例：['は', 'カレー', 'が', '好き', '。']\n",
    "        self.y = [row[1:] for row in id_data]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
    "    \n",
    "BS = 2\n",
    "dataset = MyDataset(make_word2id_seq(texts, word2id))\n",
    "dl = DataLoader(dataset, batch_size=BS, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書生成RNNの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, batch_size, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True, num_layers=self.num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def init_hidden(self, batch_size=None):\n",
    "        if not batch_size:\n",
    "            batch_size = self.batch_size\n",
    "        self.hidden_state = torch.zeros(self.num_layers, batch_size,\n",
    "                                        self.hidden_size).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, self.hidden_state = self.rnn(x, self.hidden_state)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練スクリプト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_SIZE = 300\n",
    "NUM_LAYERS = 1\n",
    "VOCAB_SIZE = len(word2id)# + 1\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, epochs, vocab_size):\n",
    "    device = model.device\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0\n",
    "        for cnt, batch in enumerate(dataloader):\n",
    "            (X_train, y_train) = batch\n",
    "            optimizer.zero_grad()\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "            model.init_hidden()\n",
    "            outputs = model(X_train)\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            y_train = y_train.reshape(-1)\n",
    "            loss = criterion(outputs, y_train)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(running_loss / cnt)\n",
    "\n",
    "        print('+', end='')\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'\\nepoch: {epoch:3}, loss: {loss:.3f}')\n",
    "\n",
    "    print(f'\\nepoch: {epoch:3}, loss: {loss:.3f}')\n",
    "    return losses\n",
    "\n",
    "# 実行は以下のコードを叩くことで可能です．\n",
    "\n",
    "EPOCHS = 1\n",
    "model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, BS, NUM_LAYERS)\n",
    "BS = 2\n",
    "dataset = MyDataset(make_word2id_seq(texts, word2id))\n",
    "dataloader = DataLoader(dataset, batch_size=BS, shuffle=True, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = train(model, dataloader, criterion, optimizer, EPOCHS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "言語モデルの学習には非常に時間がかかります．寝る前に回してみましょう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献\n",
    "\n",
    "1. [LSTMネットワークの概要](https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)\n",
    "1. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
